"""
–ü–∞—Ä—Å–µ—Ä –¥–ª—è —Å–∞–π—Ç–∞ eora.ru
–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–µ–π—Å–∞—Ö –∏ –ø—Ä–æ–µ–∫—Ç–∞—Ö –∫–æ–º–ø–∞–Ω–∏–∏
"""

import asyncio
import aiohttp
import time
import re
import unicodedata
from typing import List, Dict, Optional
from bs4 import BeautifulSoup
from loguru import logger
from dataclasses import dataclass


@dataclass
class CaseData:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–µ–π—Å–∞"""
    title: str
    description: str
    client: str
    technologies: List[str]
    url: str
    category: Optional[str] = None
    content: Optional[str] = None


class EoraScraper:
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è —Å–∞–π—Ç–∞ eora.ru"""
    
    def __init__(self, delay: float = 1.0, max_retries: int = 3):
        self.delay = delay
        self.max_retries = max_retries
        self.session: Optional[aiohttp.ClientSession] = None
    
    def _clean_text(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç Unicode-—Å–∏–º–≤–æ–ª–æ–≤ –∏ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤"""
        if not text:
            return ""
        
        # –£–¥–∞–ª—è–µ–º –Ω–µ–≤–∏–¥–∏–º—ã–µ Unicode-—Å–∏–º–≤–æ–ª—ã (–≤–∫–ª—é—á–∞—è U+200E)
        text = ''.join(char for char in text if unicodedata.category(char) != 'Cf')
        
        # –£–¥–∞–ª—è–µ–º –¥—Ä—É–≥–∏–µ –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
        text = re.sub(r'[\u200B-\u200D\uFEFF]', '', text)  # Zero-width characters
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'\s+', ' ', text)
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ
        text = text.strip()
        
        return text
        
    async def __aenter__(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä - –≤—Ö–æ–¥"""
        self.session = aiohttp.ClientSession(
            headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
        )
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä - –≤—ã—Ö–æ–¥"""
        if self.session:
            await self.session.close()
    
    async def fetch_page(self, url: str) -> Optional[str]:
        """–ü–æ–ª—É—á–∏—Ç—å HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å retry –ª–æ–≥–∏–∫–æ–π"""
        for attempt in range(self.max_retries):
            try:
                logger.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É: {url} (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1})")
                
                async with self.session.get(url, timeout=30) as response:
                    if response.status == 200:
                        content = await response.text()
                        logger.success(f"–£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞: {url}")
                        return content
                    else:
                        logger.warning(f"HTTP {response.status} –¥–ª—è {url}")
                        
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}: {e}")
                if attempt < self.max_retries - 1:
                    await asyncio.sleep(2 ** attempt)  # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
                    
        logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {url} –ø–æ—Å–ª–µ {self.max_retries} –ø–æ–ø—ã—Ç–æ–∫")
        return None
    
    def parse_case_page(self, html: str, url: str) -> Optional[CaseData]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–µ–π—Å–∞"""
        try:
            soup = BeautifulSoup(html, 'lxml')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            title = self._extract_title(soup)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
            description = self._extract_description(soup)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª–∏–µ–Ω—Ç–∞
            client = self._extract_client(soup)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏
            technologies = self._extract_technologies(soup)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∏–∑ URL
            category = self._extract_category_from_url(url)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
            content = self._extract_content(soup)
            
            return CaseData(
                title=title,
                description=description,
                client=client,
                technologies=technologies,
                url=url,
                category=category,
                content=content
            )
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {url}: {e}")
            return None
    
    def _extract_title(self, soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∫–µ–π—Å–∞"""
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
        selectors = [
            'h1',
            '.case-title',
            '.project-title',
            'h1.case-title',
            '.hero h1',
            'title'
        ]
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                title = element.get_text(strip=True)
                if title and len(title) > 5:
                    return title
        
        return "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
    
    def _extract_description(self, soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è –∫–µ–π—Å–∞"""
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –æ–ø–∏—Å–∞–Ω–∏—è
        selectors = [
            '.case-description',
            '.project-description',
            '.hero p',
            '.intro p',
            'meta[name="description"]',
            '.content p:first-of-type'
        ]
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                if selector == 'meta[name="description"]':
                    description = element.get('content', '')
                else:
                    description = element.get_text(strip=True)
                
                if description and len(description) > 20:
                    return description
        
        return "–û–ø–∏—Å–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"
    
    def _extract_client(self, soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–ª–∏–µ–Ω—Ç–∞"""
        # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –∫–ª–∏–µ–Ω—Ç–∞ –≤ —Ç–µ–∫—Å—Ç–µ
        text = soup.get_text().lower()
        
        # –°–ø–∏—Å–æ–∫ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤ –∏–∑ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è
        known_clients = [
            '–º–∞–≥–Ω–∏—Ç', 'kazanexpress', 'lamoda', 'dodo pizza', 'purina',
            'avon', 's7', 'qiwi', 'intel', 'karcher', '–Ω–µ–π—Ä–æ–Ω–µ—Ç', 'skolkovo',
            'workeat', '–¥–æ–¥–æ –ø–∏—Ü—Ü–∞', '–¥–æ–¥–æ', 'goose gaming', '—Ö–∏–º—Ä–∞—Ä', 'chemrar',
            'skinclub', 'skin club', '—Å—Ç–æ–ª–æ—Ç–æ', 'stoloto'
        ]
        
        # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        for client in known_clients:
            if client in text:
                # –£–ª—É—á—à–∞–µ–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π –∫–ª–∏–µ–Ω—Ç–æ–≤
                if client == 'dodo pizza' or client == '–¥–æ–¥–æ –ø–∏—Ü—Ü–∞' or client == '–¥–æ–¥–æ':
                    return "–î–æ–¥–æ –ü–∏—Ü—Ü–∞"
                elif client == 'workeat':
                    return "WorkEat"
                elif client == 'goose gaming':
                    return "Goose Gaming"
                elif client == '—Ö–∏–º—Ä–∞—Ä' or client == 'chemrar':
                    return "–•–∏–º—Ä–∞—Ä"
                elif client == 'skinclub' or client == 'skin club':
                    return "SkinClub"
                elif client == '—Å—Ç–æ–ª–æ—Ç–æ' or client == 'stoloto':
                    return "–°—Ç–æ–ª–æ—Ç–æ"
                else:
                    return client.title()
        
        # –ò—â–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤ –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ –±–æ–ª–µ–µ –≥–∏–±–∫–æ
        client_patterns = [
            r'–∫–ª–∏–µ–Ω—Ç[:\s]+([^\.\n]+)',
            r'–∑–∞–∫–∞–∑—á–∏–∫[:\s]+([^\.\n]+)',
            r'–¥–ª—è\s+([^\.\n]+?)(?:\.|$)',
            r'–ø—Ä–æ–µ–∫—Ç\s+–¥–ª—è\s+([^\.\n]+?)(?:\.|$)',
            r'–∫–µ–π—Å\s+–¥–ª—è\s+([^\.\n]+?)(?:\.|$)'
        ]
        
        import re
        for pattern in client_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            if matches:
                client_name = matches[0].strip()
                # –û—á–∏—â–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–ª–∏–µ–Ω—Ç–∞
                client_name = re.sub(r'[¬´¬ª""]', '', client_name)
                if len(client_name) > 2 and len(client_name) < 50:
                    return client_name.title()
        
        # –ò—â–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö –∏ –æ–ø–∏—Å–∞–Ω–∏—è—Ö
        title_text = soup.find('title')
        if title_text:
            title_lower = title_text.get_text().lower()
            for client in known_clients:
                if client in title_lower:
                    if client == 'dodo pizza' or client == '–¥–æ–¥–æ –ø–∏—Ü—Ü–∞' or client == '–¥–æ–¥–æ':
                        return "–î–æ–¥–æ –ü–∏—Ü—Ü–∞"
                    elif client == 'workeat':
                        return "WorkEat"
                    elif client == 'goose gaming':
                        return "Goose Gaming"
                    elif client == '—Ö–∏–º—Ä–∞—Ä' or client == 'chemrar':
                        return "–•–∏–º—Ä–∞—Ä"
                    elif client == 'skinclub' or client == 'skin club':
                        return "SkinClub"
                    elif client == '—Å—Ç–æ–ª–æ—Ç–æ' or client == 'stoloto':
                        return "–°—Ç–æ–ª–æ—Ç–æ"
                    else:
                        return client.title()
        
        return "–ö–ª–∏–µ–Ω—Ç –Ω–µ —É–∫–∞–∑–∞–Ω"
    
    def _extract_technologies(self, soup: BeautifulSoup) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π"""
        technologies = []
        text = soup.get_text().lower()
        
        # –°–ø–∏—Å–æ–∫ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –¥–ª—è –ø–æ–∏—Å–∫–∞
        tech_keywords = [
            'python', 'javascript', 'react', 'vue', 'angular', 'node.js',
            'machine learning', 'ai', 'neural network', 'computer vision',
            'nlp', 'chatbot', 'api', 'docker', 'kubernetes', 'postgresql',
            'mongodb', 'redis', 'elasticsearch', 'tensorflow', 'pytorch',
            'openai', 'gpt', 'transformer', 'deep learning', 'ml'
        ]
        
        for tech in tech_keywords:
            if tech in text:
                technologies.append(tech.title())
        
        return list(set(technologies))  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    
    def _extract_category_from_url(self, url: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑ URL"""
        if '/cases/' in url:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∏–∑ URL
            parts = url.split('/cases/')
            if len(parts) > 1:
                category = parts[1].split('/')[0]
                # –£–ª—É—á—à–∞–µ–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
                category = category.replace('-', ' ').title()
                
                # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
                category_mapping = {
                    'workeat whatsapp bot': 'WorkEat WhatsApp Bot',
                    'dodo pizza robot analitik otzyvov': '–î–æ–¥–æ –ü–∏—Ü—Ü–∞ - –†–æ–±–æ—Ç-–∞–Ω–∞–ª–∏—Ç–∏–∫',
                    'dodo pizza bot dlya telefonii': '–î–æ–¥–æ –ü–∏—Ü—Ü–∞ - –ë–æ—Ç –¥–ª—è —Ç–µ–ª–µ—Ñ–æ–Ω–∏–∏',
                    'goosegaming algoritm dlya ocenki igrokov': 'Goose Gaming - –ê–ª–≥–æ—Ä–∏—Ç–º –æ—Ü–µ–Ω–∫–∏ –∏–≥—Ä–æ–∫–æ–≤',
                    'lamoda systema segmentacii i poiska po pohozhey odezhde': 'Lamoda - –°–∏—Å—Ç–µ–º–∞ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏',
                    'zeptolab skazki pro amnyama dlya sberbox': 'ZeptoLab - –°–∫–∞–∑–∫–∏ –¥–ª—è SberBox',
                    'assistenty dlya gorodov': '–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç—ã –¥–ª—è –≥–æ—Ä–æ–¥–æ–≤',
                    'navyki dlya golosovyh assistentov': '–ù–∞–≤—ã–∫–∏ –¥–ª—è –≥–æ–ª–æ—Å–æ–≤—ã—Ö –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤',
                    'avtomatizaciya v promyshlennosti': '–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –≤ –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ—Å—Ç–∏',
                    'kompyuternoe zrenie i ii': '–ö–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ –∏ –ò–ò',
                    'razrabotka chat botov': '–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ —á–∞—Ç-–±–æ—Ç–æ–≤',
                    'avtomatizaciya kontakt centrov': '–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ç–∞–∫—Ç-—Ü–µ–Ω—Ç—Ä–æ–≤'
                }
                
                if category.lower() in category_mapping:
                    return category_mapping[category.lower()]
                
                return category
        
        return "–û–±—â–∏–µ –∫–µ–π—Å—ã"
    
    def _extract_content(self, soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        for element in soup(['script', 'style', 'nav', 'header', 'footer']):
            element.decompose()
        
        # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
        content_selectors = [
            '.content',
            '.case-content',
            '.project-content',
            'main',
            'article',
            '.text-content'
        ]
        
        for selector in content_selectors:
            element = soup.select_one(selector)
            if element:
                content = element.get_text(strip=True)
                return self._clean_text(content)
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä, –±–µ—Ä–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç
        content = soup.get_text(strip=True)
        return self._clean_text(content)
    
    async def scrape_cases(self, urls: List[str]) -> List[CaseData]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö –∫–µ–π—Å–æ–≤"""
        cases = []
        
        for i, url in enumerate(urls):
            logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–µ–π—Å {i+1}/{len(urls)}: {url}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
            html = await self.fetch_page(url)
            if not html:
                continue
            
            # –ü–∞—Ä—Å–∏–º –∫–µ–π—Å
            case = self.parse_case_page(html, url)
            if case:
                cases.append(case)
                logger.success(f"–£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∫–µ–π—Å: {case.title}")
            else:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –∫–µ–π—Å: {url}")
            
            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            if i < len(urls) - 1:  # –ù–µ –∂–¥–µ–º –ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
                await asyncio.sleep(self.delay)
        
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(cases)} –∫–µ–π—Å–æ–≤ –∏–∑ {len(urls)}")
        return cases


# –°–ø–∏—Å–æ–∫ URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏–∑ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è
EORA_CASES_URLS = [
    "https://eora.ru/cases/promyshlennaya-bezopasnost",
    "https://eora.ru/cases/lamoda-systema-segmentacii-i-poiska-po-pohozhey-odezhde",
    "https://eora.ru/cases/navyki-dlya-golosovyh-assistentov/karas-golosovoy-assistent",
    "https://eora.ru/cases/assistenty-dlya-gorodov",
    "https://eora.ru/cases/avtomatizaciya-v-promyshlennosti/chemrar-raspoznovanie-molekul",
    "https://eora.ru/cases/zeptolab-skazki-pro-amnyama-dlya-sberbox",
    "https://eora.ru/cases/goosegaming-algoritm-dlya-ocenki-igrokov",
    "https://eora.ru/cases/dodo-pizza-robot-analitik-otzyvov",
    "https://eora.ru/cases/ifarm-nejroset-dlya-ferm",
    "https://eora.ru/cases/zhivibezstraha-navyk-dlya-proverki-rodinok",
    "https://eora.ru/cases/sportrecs-nejroset-operator-sportivnyh-translyacij",
    "https://eora.ru/cases/avon-chat-bot-dlya-zhenshchin",
    "https://eora.ru/cases/navyki-dlya-golosovyh-assistentov/navyk-dlya-proverki-loterejnyh-biletov",
    "https://eora.ru/cases/computer-vision/iss-analiz-foto-avtomobilej",
    "https://eora.ru/cases/purina-master-bot",
    "https://eora.ru/cases/skinclub-algoritm-dlya-ocenki-veroyatnostej",
    "https://eora.ru/cases/skolkovo-chat-bot-dlya-startapov-i-investorov",
    "https://eora.ru/cases/purina-podbor-korma-dlya-sobaki",
    "https://eora.ru/cases/purina-navyk-viktorina",
    "https://eora.ru/cases/dodo-pizza-pilot-po-avtomatizacii-kontakt-centra",
    "https://eora.ru/cases/dodo-pizza-avtomatizaciya-kontakt-centra",
    "https://eora.ru/cases/icl-bot-sufler-dlya-kontakt-centra",
    "https://eora.ru/cases/s7-navyk-dlya-podbora-aviabiletov",
    "https://eora.ru/cases/workeat-whatsapp-bot",
    "https://eora.ru/cases/absolyut-strahovanie-navyk-dlya-raschyota-strahovki",
    "https://eora.ru/cases/kazanexpress-poisk-tovarov-po-foto",
    "https://eora.ru/cases/kazanexpress-sistema-rekomendacij-na-sajte",
    "https://eora.ru/cases/intels-proverka-logotipa-na-plagiat",
    "https://eora.ru/cases/karcher-viktorina-s-voprosami-pro-uborku",
    "https://eora.ru/cases/chat-boty/purina-friskies-chat-bot-na-sajte",
    "https://eora.ru/cases/nejroset-segmentaciya-video",
    "https://eora.ru/cases/chat-boty/essa-nejroset-dlya-generacii-rolikov",
    "https://eora.ru/cases/qiwi-poisk-anomalij",
    "https://eora.ru/cases/frisbi-nejroset-dlya-raspoznavaniya-pokazanij-schetchikov",
    "https://eora.ru/cases/skazki-dlya-gugl-assistenta",
    "https://eora.ru/cases/chat-boty/hr-bot-dlya-magnit-kotoriy-priglashaet-na-sobesedovanie"
]


async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø–∞—Ä—Å–µ—Ä–∞"""
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞ EORA...")
    
    async with EoraScraper(delay=1.0) as scraper:
        # –ü–∞—Ä—Å–∏–º –ø–µ—Ä–≤—ã–µ 3 –∫–µ–π—Å–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        test_urls = EORA_CASES_URLS[:3]
        cases = await scraper.scrape_cases(test_urls)
        
        # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for i, case in enumerate(cases, 1):
            logger.info(f"\n--- –ö–µ–π—Å {i} ---")
            logger.info(f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {case.title}")
            logger.info(f"–ö–ª–∏–µ–Ω—Ç: {case.client}")
            logger.info(f"–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏: {', '.join(case.technologies)}")
            logger.info(f"URL: {case.url}")
            logger.info(f"–û–ø–∏—Å–∞–Ω–∏–µ: {case.description[:100]}...")
    
    logger.info("‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω!")


if __name__ == "__main__":
    asyncio.run(main()) 