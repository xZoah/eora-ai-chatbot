"""
–ú–æ–¥—É–ª—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä—Å–µ—Ä–∞
"""

import json
import asyncio
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any
from loguru import logger

from .scraper import EoraScraper, CaseData, EORA_CASES_URLS


class DataManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä—Å–µ—Ä–∞"""
    
    def __init__(self, output_dir: str = "data"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
    def save_cases_to_json(self, cases: List[CaseData], filename: str = None) -> str:
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–µ–π—Å—ã –≤ JSON —Ñ–∞–π–ª"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"eora_cases_{timestamp}.json"
        
        filepath = self.output_dir / filename
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º CaseData –≤ —Å–ª–æ–≤–∞—Ä–∏
        cases_data = []
        for case in cases:
            case_dict = {
                "title": case.title,
                "description": case.description,
                "client": case.client,
                "technologies": case.technologies,
                "url": case.url,
                "category": case.category,
                "content": case.content,
                "parsed_at": datetime.now().isoformat()
            }
            cases_data.append(case_dict)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ JSON
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(cases_data, f, ensure_ascii=False, indent=2)
        
        logger.success(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(cases)} –∫–µ–π—Å–æ–≤ –≤ {filepath}")
        return str(filepath)
    
    def load_cases_from_json(self, filename: str) -> List[Dict[str, Any]]:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∫–µ–π—Å—ã –∏–∑ JSON —Ñ–∞–π–ª–∞"""
        filepath = self.output_dir / filename
        
        if not filepath.exists():
            logger.error(f"–§–∞–π–ª {filepath} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return []
        
        with open(filepath, 'r', encoding='utf-8') as f:
            cases_data = json.load(f)
        
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(cases_data)} –∫–µ–π—Å–æ–≤ –∏–∑ {filepath}")
        return cases_data
    
    def get_statistics(self, cases: List[CaseData]) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∫–µ–π—Å–∞–º"""
        if not cases:
            return {}
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–∏–µ–Ω—Ç–∞–º
        clients = {}
        for case in cases:
            client = case.client
            clients[client] = clients.get(client, 0) + 1
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è–º
        technologies = {}
        for case in cases:
            for tech in case.technologies:
                technologies[tech] = technologies.get(tech, 0) + 1
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        categories = {}
        for case in cases:
            category = case.category
            categories[category] = categories.get(category, 0) + 1
        
        return {
            "total_cases": len(cases),
            "clients": clients,
            "technologies": technologies,
            "categories": categories,
            "avg_content_length": sum(len(case.content or "") for case in cases) / len(cases)
        }


async def run_full_parsing(output_filename: str = None) -> str:
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å –ø–æ–ª–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö –∫–µ–π—Å–æ–≤"""
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ EORA...")
    
    # –°–æ–∑–¥–∞–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä –¥–∞–Ω–Ω—ã—Ö
    data_manager = DataManager()
    
    # –ü–∞—Ä—Å–∏–º –≤—Å–µ –∫–µ–π—Å—ã
    async with EoraScraper(delay=1.0) as scraper:
        cases = await scraper.scrape_cases(EORA_CASES_URLS)
    
    if not cases:
        logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ")
        return ""
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ JSON
    filepath = data_manager.save_cases_to_json(cases, output_filename)
    
    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    stats = data_manager.get_statistics(cases)
    logger.info("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞:")
    logger.info(f"–í—Å–µ–≥–æ –∫–µ–π—Å–æ–≤: {stats['total_cases']}")
    logger.info(f"–ö–ª–∏–µ–Ω—Ç–æ–≤: {len(stats['clients'])}")
    logger.info(f"–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–π: {len(stats['technologies'])}")
    logger.info(f"–ö–∞—Ç–µ–≥–æ—Ä–∏–π: {len(stats['categories'])}")
    
    # –¢–æ–ø –∫–ª–∏–µ–Ω—Ç–æ–≤
    top_clients = sorted(stats['clients'].items(), key=lambda x: x[1], reverse=True)[:5]
    logger.info("\nüèÜ –¢–æ–ø –∫–ª–∏–µ–Ω—Ç–æ–≤:")
    for client, count in top_clients:
        logger.info(f"  {client}: {count} –∫–µ–π—Å–æ–≤")
    
    # –¢–æ–ø —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π
    top_tech = sorted(stats['technologies'].items(), key=lambda x: x[1], reverse=True)[:5]
    logger.info("\nüîß –¢–æ–ø —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π:")
    for tech, count in top_tech:
        logger.info(f"  {tech}: {count} –∫–µ–π—Å–æ–≤")
    
    return filepath


async def run_test_parsing(num_cases: int = 3) -> str:
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–µ–π—Å–æ–≤"""
    logger.info(f"üß™ –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ ({num_cases} –∫–µ–π—Å–æ–≤)...")
    
    # –°–æ–∑–¥–∞–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä –¥–∞–Ω–Ω—ã—Ö
    data_manager = DataManager()
    
    # –ü–∞—Ä—Å–∏–º —Ç–µ—Å—Ç–æ–≤—ã–µ –∫–µ–π—Å—ã
    test_urls = EORA_CASES_URLS[:num_cases]
    async with EoraScraper(delay=1.0) as scraper:
        cases = await scraper.scrape_cases(test_urls)
    
    if not cases:
        logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ")
        return ""
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ JSON
    filepath = data_manager.save_cases_to_json(cases, f"test_cases_{num_cases}.json")
    
    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    logger.info("\nüìã –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞:")
    for i, case in enumerate(cases, 1):
        logger.info(f"\n--- –ö–µ–π—Å {i} ---")
        logger.info(f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {case.title}")
        logger.info(f"–ö–ª–∏–µ–Ω—Ç: {case.client}")
        logger.info(f"–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏: {', '.join(case.technologies)}")
        logger.info(f"URL: {case.url}")
        logger.info(f"–û–ø–∏—Å–∞–Ω–∏–µ: {case.description[:100]}...")
    
    return filepath


def validate_parsed_data(cases: List[CaseData]) -> Dict[str, List[str]]:
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
    issues = {
        "missing_title": [],
        "missing_description": [],
        "missing_client": [],
        "short_content": []
    }
    
    for i, case in enumerate(cases):
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
        if not case.title or case.title == "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è":
            issues["missing_title"].append(f"–ö–µ–π—Å {i+1}: {case.url}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
        if not case.description or case.description == "–û–ø–∏—Å–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ":
            issues["missing_description"].append(f"–ö–µ–π—Å {i+1}: {case.url}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª–∏–µ–Ω—Ç–∞
        if not case.client or case.client == "–ö–ª–∏–µ–Ω—Ç –Ω–µ —É–∫–∞–∑–∞–Ω":
            issues["missing_client"].append(f"–ö–µ–π—Å {i+1}: {case.url}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        if case.content and len(case.content) < 100:
            issues["short_content"].append(f"–ö–µ–π—Å {i+1}: {case.url} ({len(case.content)} —Å–∏–º–≤–æ–ª–æ–≤)")
    
    return issues


async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø–∞—Ä—Å–µ—Ä–∞"""
    import argparse
    
    parser = argparse.ArgumentParser(description="–ü–∞—Ä—Å–µ—Ä –∫–µ–π—Å–æ–≤ EORA")
    parser.add_argument("--test", action="store_true", help="–ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–π –ø–∞—Ä—Å–∏–Ω–≥")
    parser.add_argument("--num-test", type=int, default=3, help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–µ–π—Å–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
    parser.add_argument("--output", type=str, help="–ò–º—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞")
    
    args = parser.parse_args()
    
    if args.test:
        filepath = await run_test_parsing(args.num_test)
    else:
        filepath = await run_full_parsing(args.output)
    
    if filepath:
        logger.success(f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω! –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {filepath}")
    else:
        logger.error("‚ùå –ü–∞—Ä—Å–∏–Ω–≥ –Ω–µ —É–¥–∞–ª—Å—è")


if __name__ == "__main__":
    asyncio.run(main()) 